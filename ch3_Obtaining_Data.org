* Obtaining Data

Data can be obtained in several ways - 

- Downloading it from a server
- Querying a database
- Connecting to a web api
- Getting a compressed file, such as a binary or excel file 

** 3.1 | Overview

In this chapter we learn how to 
- Obtain data from the internet
- Query databases
- Connect to web apis
- Decompress files
- Convert excel spreadsheets into usable data

** 3.2 | Copying Local Files 

#+BEGIN_SRC bash :dir ~/Desktop/log/ds_cmd/
mkdir data
cp -r ~/data-science-at-the-command-line/data/* data/
#+END_SRC

#+RESULTS:

** 3.3 | Decompressing Files 

If the original dataset is very large or a collection of many files, the file may be a compressed archive. 

Common file extensions of compressed archives and their decompression tools include

- .tar.gz & tar
- .zip & unzip
- .rar & unrar

For example, in order to extract a file named logs.tar.gz, we would use: 

#+BEGIN_SRC
cd ~/dir/folder 
tar -xzvf /data/logs.tar.gz
#+END_SRC

In this case, the four command line arguments x z v f specify that tar should:

- extract files from an archive
- use gzip as the decompression algorithm
- be verbose
- use the file logs.tar.gz 

Rather than remembering the different command line tools and their options, there is a handy script called unpack which will decompress many different formats. unpack looks at the extension of the file you want to decompress and calls the appropriate command line tool.

#+BEGIN_SRC bash :results verbatim raw
cat ~/bin/unpack
#+END_SRC

#+RESULTS:
#!/usr/bin/env bash
# unpack: Extract common file formats
 
# Dependencies: unrar, unzip, p7zip-full

# Author: Patrick Brisbin
# From: http://linuxtidbits.wordpress.com/2009/08/04/week-of-bash-scripts-extract/

# Display usage if no parameters given
if [[ -z "$@" ]]; then
	echo " ${0##*/} <archive> - extract common file formats)"
	exit
fi
 
# Required program(s)
req_progs=(7z unrar unzip)
for p in ${req_progs[@]}; do
	hash "$p" 2>&- || \
	{ echo >&2 " Required program \"$p\" not installed."; exit 1; }
done
 
# Test if file exists
if [ ! -f "$@" ]; then
	echo "File "$@" doesn't exist"
	exit
fi
 
# Extract file by using extension as reference
case "$@" in
	*.7z ) 7z x "$@" ;;
	*.tar.bz2 ) tar xvjf "$@" ;;
	*.bz2 ) bunzip2 "$@" ;;
	*.deb ) ar vx "$@" ;;
	*.tar.gz ) tar xvf "$@" ;;
	*.gz ) gunzip "$@" ;;
	*.tar ) tar xvf "$@" ;;
	*.tbz2 ) tar xvjf "$@" ;;
	*.tar.xz ) tar xvf "$@" ;;
	*.tgz ) tar xvzf "$@" ;;
	*.rar ) unrar x "$@" ;;
	*.zip ) unzip "$@" ;;
	*.Z ) uncompress "$@" ;;
	* ) echo " Unsupported file format" ;;
esac

** 3.4 | Converting Microsoft Excel Spreadsheets 

There is a command line tool called in2csv which is able to convert microsoft excel spreadsheets into CSV files. 

We can demonstrate in2csv using a spreadsheet of the top 250 movies from IMDB. 

To extract its data, we invoke in2csv as follows

#+BEGIN_SRC bash :dir ~/Desktop/log/ds_cmd/
cd data/ch03/data 

in2csv imdb-250.xlsx > imdb250.csv
#+END_SRC

#+RESULTS:

The format of the file is automatically determined by the extensions (.xslx in this case). If we were to pipe the data into in2csv, we would have to specify the format explicitly.

Let's look at the data 

#+BEGIN_SRC bash :dir ~/Desktop/log/ds_cmd/data/ch03/data
in2csv imdb-250.xlsx | head | csvcut -c Title,Year,Rating | csvlook
#+END_SRC

#+RESULTS:
|   |   | Title                                   |         |           | Year   |         |         | Rating |       |   |       |   |   |   |   |
|   |   | --------------------------------------- |         |           | -----  |         |         | ------ |       |   |       |   |   |   |   |
|   |   | Sherlock                                | Jr.     | (1924)    |        |         | 1,924   |        |       | 8 |       |   |   |   |   |
|   |   | The                                     | Passion | of        | Joan   | of      | Arc     | (1928) |       |   | 1,928 |   |   | 8 |   |
|   |   | His                                     | Girl    | Friday    | (1940) |         |         | 1,940  |       |   |     8 |   |   |   |   |
|   |   | Tokyo                                   | Story   | (1953)    |        |         | 1,953   |        |       | 8 |       |   |   |   |   |
|   |   | The                                     | Man     | Who       | Shot   | Liberty | Valance | (1962) |       |   | 1,962 |   |   | 8 |   |
|   |   | Persona                                 | (1966)  |           |        | 1,966   |         |        |     8 |   |       |   |   |   |   |
|   |   | Stalker                                 | (1979)  |           |        | 1,979   |         |        |     8 |   |       |   |   |   |   |
|   |   | Fanny                                   | and     | Alexander | (1982) |         |         | 1,982  |       |   |     8 |   |   |   |   |
|   |   | Beauty                                  | and     | the       | Beast  | (1991)  |         |        | 1,991 |   |       | 8 |   |   |   |

A spreadsheet can contain multiple worksheets. By default, in2csv extracts the first worksheet. To extract a different worksheet, we need to pass the name of the worksheet to the --sheet option. 

The tools in2csv, csvcut, and csvlook are a part of csvkit. 

** 3.5 | Querying Relational Databases 

We can connect to most databases via command line, but they all have differing APIs. We can also use the cmd line tool sql2csv, which supports select, insert, update, and delete queries.

To select a specific set of data from an sqlite db named iris.db, we can invoke sql2csv as follows:

#+BEGIN_SRC bash
sql2csv --db 'sqlite:///data/iris.db' --query 'SELECT * FROM iris WHERE sepal_length > 7.5'
#+END_SRC

#+RESULTS:

** 3.6 | Downloading from the Internet 

The swiss army knife for downloading from the internet is curl. We can specify a url for curl, such as 

#+BEGIN_SRC bash :results verbatim raw
curl -s http://www.gutenberg.org/files/76/76-0.txt | head -n 10
#+END_SRC

#+RESULTS:
﻿
The Project Gutenberg EBook of Adventures of Huckleberry Finn, Complete
by Mark Twain (Samuel Clemens)

This eBook is for the use of anyone anywhere at no cost and with almost
no restrictions whatsoever. You may copy it, give it away or re-use
it under the terms of the Project Gutenberg License included with this
eBook or online at www.gutenberg.net

Title: Adventures of Huckleberry Finn, Complete

The -s stands for silent. If we don't specify this, we get a progress bar.

#+BEGIN_SRC bash :results verbatim
curl http://www.gutenberg.org/files/76/76-0.txt | head -n 10
#+END_SRC

#+RESULTS:
#+begin_example
﻿
The Project Gutenberg EBook of Adventures of Huckleberry Finn, Complete
by Mark Twain (Samuel Clemens)

This eBook is for the use of anyone anywhere at no cost and with almost
no restrictions whatsoever. You may copy it, give it away or re-use
it under the terms of the Project Gutenberg License included with this
eBook or online at www.gutenberg.net

Title: Adventures of Huckleberry Finn, Complete
#+end_example

If we save the data to a file, we don't need to specify the -s command

#+BEGIN_SRC bash
curl -s http://www.gutenberg.org/files/76/76-0.txt > finn.txt
#+END_SRC

#+RESULTS:

We can also save the data by specifying the -o command

#+BEGIN_SRC bash
curl -s http://www.gutenberg.org/files/76/76-0.txt -o finn.txt
#+END_SRC

#+RESULTS:

When the url is password protected, we can specify as such: 

#+BEGIN_SRC bash
curl -u username:password ftp://host/file
#+END_SRC

#+RESULTS:

If the specified url is a directory, curl will list the contents of the directory. If we access a shortened url, such as bit.ly, with curl we need to specify the -L or --location option in order to be redirected

#+BEGIN_SRC bash
curl -L j.mp/locatbbar
#+END_SRC

#+RESULTS:


If we don't specify the -L flag, we will get something like

#+BEGIN_SRC bash :results verbatim
curl j.mp/locatbbar
#+END_SRC

#+RESULTS:
: <html>
: <head><title>Bitly</title></head>
: <body><a href="http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio">moved here</a></body>
: </html>

By specifying the -I or --head option, curl fetches only the HTTP header of the response.

#+BEGIN_SRC bash :results verbatim
curl -I j.mp/locatbbar
#+END_SRC

#+RESULTS:
: HTTP/1.1 301 Moved Permanently
: Server: nginx
: Date: Sat, 22 Jun 2019 03:18:36 GMT
: Content-Type: text/html; charset=utf-8
: Content-Length: 170
: Connection: keep-alive
: Cache-Control: private, max-age=90
: Location: http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio
: 

In summary, curl is used to download data from the internet. 

- \-s supresses progress meter 
- \-u specifies a username and password
-  \-L automatically follows redirects 

** 3.7 | Calling a Web API 

Web APIs often return data in a structured format, such as xml or json. We can use jq to deal with this 

#+BEGIN_SRC bash :results verbatim raw
curl -s https://randomuser.me/api/1.2 | jq
#+END_SRC

#+RESULTS:

Some APIs require you to log in using the OAuth protocol. There is a cmd line tool called curlicue that assists. It helps navigate the authentication. 

For example, with the twitter api we might run:

#+BEGIN_SRC bash
curlicue-setup \
'https://api.twitter.com/oauth/request_token' \
'https://api.twitter.com/oauth/authorize?oauth_token=$oauth_token' \
'https://api.twitter.com/oauth/access_token' \
credentials
curlicue -f credentials \
'https://api.twitter.com/1/statuses/home_timeline.xml'
#+END_SRC

#+RESULTS:
